\section{Related work}
% Hvad har andre folk gjort med problemet, nævn nogle stykker og forklar hvad de har gjort
% Slut af med tmseg og beskrev hvad de har gjort mere beskrivende.
% Hvad har andre folk gjort med machine learning problemer baseret på sekvens data

\subsection{Transmembrane Protein Prediction}
% "A hidden Markov model for predicting transmembrane helices in protein sequences"\cite{tmhmm} fra bioinformatik-kurset. 
		% At TM prediction er muligt og kan gøres "simpelt" med HMM
% 
% i TMSEG nævnes MEMSAT. måske skal vi skrive noget om den
% Der var en artikel der hedder HTP der virker interressant
% Hvilke modeller eksisterer allerede og bliver brugt pt. 

A lot of different approaches have been proposed to solve the problem, different types of machine 
learning but also non machine learning approaches like the one proposed in \cite{Heijne2} where 
where they use hydrophobicity analysis to predict the structure. For machine learning based 
approaches is there a wide range of models, from TMHMM \cite{tmhmm}, where they used a \gls{hmm},
over \gls{svm} based models like MEMSAT-SVM \cite{memsat-svm}, to models like MEMSAT3 \cite{memsat3}
and TMSEG \cite{tmseg} which used \glspl{ann}. TMSEG also used different machine learning methods 
and is the model with the best results from those listed here and is explained more in next section.

\subsubsection{TMSEG}
In TMSEG they divide the problem into four sub problems, called steps. 
Most notable is the separation of the prediction of the \glspl{tmh} and the inside/outside topology. 

In the first step they assigns a probability distribution of \gls{tmh}, signal peptide and non-transmembrane 
to each aminoacid. To do this they used a \gls{rf} with a sliding window. 

Step 2 is a filtering of the probability scores from last step. This is done by first smoothing the scores 
by assigning each aminoacid the median value in a window. After this each aminoacid is assigned to 
one of the classes based on the score with a empirically determined penalty applied to each class.
They also removes predicted \gls{tmh} if it is too short.

In the third step they use an \gls{ann} to correct the predicted \glspl{tmh}. This is done by extracting a 
number of features from the segment where the predicted \gls{tmh} is and a couple of segments moved and 
resized a little, the probability for each of these segments is a \gls{tmh} is then computed from the features
and the segment with the highest probability is the corrected prediction. This step also splits 
\glspl{tmh} if the split gives higher probabilities. 

In the last step the inside/outside topology is predicted by a \gls{rf}. They use segments from around 
the start and end of predicted \glspl{tmh}, then they extracts features from the segments and predict which 
side is inside and which is outside.

\subsection{Long Short-Term Memory Network}
%Find nogle artikler der laver noget nice med lstm

A lot af recent exiting advances in areas where the data is sequence based 
has been done with \glspl{lstm}. Areas such as language modelling \cite{JozefowiczEtAl, ShazeerEtAl},
speech recognition \cite{XiongEtAl}, machine translation \cite{WuEtAl} and many more, have had the 
state of the art moved by the use of \glspl{lstm}. 

% Åbenbart bruger de slet ikke lstm til deres branching eller expert layers. Det virker ikke til de bruger lstm'er på en ny måde
%In \cite{ShazeerEtAl} a \gls{lstm} is used as a gating network to enable conditional computations of a 
%much larger network. 

In \cite{JozefowiczEtAl} they explore different model architectures for language modelling 
on very large datasets, and comes to the conclusion that \glspl{lstm} is the best of the models
they tried and the bigger layers they used, the better results they got. Most of their work is
in decreasing the number of computations necessary to train such large \glspl{lstm}. 
 
 