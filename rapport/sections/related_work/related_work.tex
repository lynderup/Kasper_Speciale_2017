\section{Related work}
% Hvad har andre folk gjort med problemet, nævn nogle stykker og forklar hvad de har gjort
% Slut af med tmseg og beskrev hvad de har gjort mere beskrivende.
% Hvad har andre folk gjort med machine learning problemer baseret på sekvens data

\subsection{Transmembrane Protein Prediction}
% "A hidden Markov model for predicting transmembrane helices in protein sequences"\cite{tmhmm} fra bioinformatik-kurset. 
		% At TM prediction er muligt og kan gøres "simpelt" med HMM
% 
% i TMSEG nævnes MEMSAT. måske skal vi skrive noget om den

% Der var en artikel der hedder HTP der virker interressant

% Hvilke modeller eksisterer allerede og bliver brugt pt. 

\subsubsection{TMSEG}
In TMSEG\cite{tmseg} they divide the problem into four sub problems, called steps. 
Most notable is the separation of the prediction of the \glspl{tmh} and the inside/outside topology. 

In the first step they assigns a probability distribution of \gls{tmh}, signal peptide and non-transmembrane 
to each aminoacid. To do this they used a \gls{rf} with a sliding window. 

Step 2 is a filtering of the probability scores from last step. This is done by first smoothing the scores 
by assigning each aminoacid the median value in a window. After this each aminoacid is assigned to 
one of the classes based on the score with a empirically determined penalty applied to each class.
They also removes predicted \gls{tmh} if it is too short.

In the third step they use an \gls{ann} to correct the predicted \glspl{tmh}. This is done by extracting a 
number of features from the segment where the predicted \gls{tmh} is and a couple of segments moved and 
resized a little, the probability for each of these segments is a \gls{tmh} is then computed from the features
and the segment with the highest probability is the corrected prediction. This step also splits 
\glspl{tmh} if the split gives higher probabilities. 

In the last step the inside/outside topology is predicted by a \gls{rf}. They use segments from around 
the start and end of predicted \glspl{tmh}, then they extracts features from the segments and predict which 
side is inside and which is outside.

\subsection{Long Short-Term Memory Network}
%Find nogle artikler der laver noget nice med lstm

A lot af recent exiting advances in areas where the data is inherently sequence based 
have been done with \glspl{lstm}. Areas such as language modelling \cite{JozefowiczEtAl, ShazeerEtAl},
speech recognition \cite{XiongEtAl}, machine translation \cite{WuEtAl} and many more, have had the 
state of the art moved by the use of \glspl{lstm}. 

% Åbenbart bruger de slet ikke lstm til deres branching eller expert layers. Det virker ikke til de bruger lstm'er på en ny måde
%In \cite{ShazeerEtAl} a \gls{lstm} is used as a gating network to enable conditional computations of a 
%much larger network. 

In \cite{JozefowiczEtAl} they explore different model architectures for language modelling 
on very large datasets, and comes to the conclusion that \glspl{lstm} is the best of the models
they tried and the bigger layers they used, the better results they got. Most of their work is
in decreasing the number of computations necessary to train such large \glspl{lstm}. 
Amongst other they used a approximation of the softmax layer. Due to the large number of 
classes in langauge modelling the softmax layer becomes very large and have a large influence 
on the number of computations. 