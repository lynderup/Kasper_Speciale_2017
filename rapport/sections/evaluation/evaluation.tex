\section{Evaluation}

\subsection{Results}

\begin{table}
	\centering 
	\begin{tabular}{l|c|c}
		Model & Precision & Recall \\ \hline
		Step1 w/o PSSMs & $81 \pm 0$ & $83 \pm 0$ \\
		Step1 w PSSMs & $81 \pm 0$ & $83 \pm 0$ \\
		Step2 w/o PSSMs & $81 \pm 0$ & $82 \pm 0$ \\
		Step2 w PSSMs & $81 \pm 0$ & $82 \pm 0$ \\
		Step3 w/o PSSMs & $73 \pm 0$ & $84 \pm 0$ \\
		Step3 w PSSMs & $73 \pm 0$ & $84 \pm 0$ \\
		HMM & $72 \pm 0$ & $31 \pm 0$ \\
	\end{tabular}
	\caption{Precision and recall for the position based measurement.}
	\label{tab:char}
\end{table}

The precision and recall for the position based measurement is shown in 
Table \ref{tab:char}. The model gives a moderately improvement to precision
in comparison with the \gls{hmm} and a massively improvement to recall.
Step 2 and 3 does almost nothing for the result. This is because the changes
they do is only a tiny part of all positions and therefore have little to 
no impact on the result of this measurement. 

\begin{table}
	\centering 
	\begin{subtable}[]{\textwidth}
		\begin{tabular}{l|c|c|c|c|c|c|c|c} 
			Model & \multicolumn{2}{|c}{10\%}& \multicolumn{2}{|c}{25\%}& \multicolumn{2}{|c}{50\%}& \multicolumn{2}{|c}{75\%}\\ 
			& Precis & Recall & Precis & Recall & Precis & Recall & Precis & Recall \\ \hline 
			Step 1 & $ 89 \pm 1 $ & $ 92 \pm 2 $ & $ 88 \pm 2 $ & $ 90 \pm 2 $ & $ 81 \pm 3 $ & $ 84 \pm 3 $ & $ 62 \pm 3 $ & $ 63 \pm 4 $ \\ 
			w PSSMs& $ 89 \pm 1 $ & $ 92 \pm 2 $ & $ 88 \pm 2 $ & $ 90 \pm 2 $ & $ 81 \pm 3 $ & $ 84 \pm 3 $ & $ 62 \pm 3 $ & $ 63 \pm 4 $ \\
			Step 2 & $ 92 \pm 1 $ & $ 94 \pm 2 $ & $ 92 \pm 1 $ & $ 94 \pm 2 $ & $ 85 \pm 2 $ & $ 87 \pm 2 $ & $ 64 \pm 3 $ & $ 65 \pm 3 $ \\ 
			w PSSMs& $ 92 \pm 1 $ & $ 94 \pm 2 $ & $ 92 \pm 1 $ & $ 94 \pm 2 $ & $ 85 \pm 2 $ & $ 87 \pm 2 $ & $ 64 \pm 3 $ & $ 65 \pm 3 $ \\ 
			Step 3 & $ 92 \pm 1 $ & $ 92 \pm 2 $ & $ 91 \pm 1 $ & $ 91 \pm 2 $ & $ 80 \pm 2 $ & $ 80 \pm 3 $ & $ 42 \pm 4 $ & $ 42 \pm 4 $ \\
			w PSSMs& $ 92 \pm 1 $ & $ 92 \pm 2 $ & $ 91 \pm 1 $ & $ 91 \pm 2 $ & $ 80 \pm 2 $ & $ 80 \pm 3 $ & $ 42 \pm 4 $ & $ 42 \pm 4 $ \\  
		\end{tabular}
		\caption{Precision and recall for 10\%, 25\%, 50\% and 75\% overlap of true \gls{tmh} and predicted \gls{tmh}.}
		\label{tab:overlap}
	\end{subtable}
	
	\begin{subtable}[]{\textwidth}
		\begin{tabular}{l|c|c|c|c|c|c|c|c} 
			Model & \multicolumn{2}{|c}{Below 10}& \multicolumn{2}{|c}{Below 5}& \multicolumn{2}{|c}{Below 2}& \multicolumn{2}{|c}{Equal 0}\\ 
			& Precis & Recall & Precis & Recall & Precis & Recall & Precis & Recall \\ \hline 
			Step 1 & $ 80 \pm 3 $ & $ 82 \pm 3 $ & $ 64 \pm 3 $ & $ 66 \pm 3 $ & $ 26 \pm 3 $ & $ 27 \pm 4 $ & $  2 \pm 1 $ & $  2 \pm 1 $ \\ 
			w PSSMs& $ 80 \pm 3 $ & $ 82 \pm 3 $ & $ 64 \pm 3 $ & $ 66 \pm 3 $ & $ 26 \pm 3 $ & $ 27 \pm 4 $ & $  2 \pm 1 $ & $  2 \pm 1 $ \\
			Step 2 & $ 84 \pm 2 $ & $ 86 \pm 2 $ & $ 67 \pm 3 $ & $ 69 \pm 3 $ & $ 27 \pm 3 $ & $ 28 \pm 3 $ & $  2 \pm 1 $ & $  2 \pm 1 $ \\ 
			w PSSMs& $ 84 \pm 2 $ & $ 86 \pm 2 $ & $ 67 \pm 3 $ & $ 69 \pm 3 $ & $ 27 \pm 3 $ & $ 28 \pm 3 $ & $  2 \pm 1 $ & $  2 \pm 1 $ \\
			Step 3 & $ 77 \pm 2 $ & $ 77 \pm 3 $ & $ 49 \pm 3 $ & $ 49 \pm 4 $ & $ 18 \pm 2 $ & $ 18 \pm 2 $ & $  1 \pm 1 $ & $  1 \pm 1 $ \\ 
			w PSSMs& $ 77 \pm 2 $ & $ 77 \pm 3 $ & $ 49 \pm 3 $ & $ 49 \pm 4 $ & $ 18 \pm 2 $ & $ 18 \pm 2 $ & $  1 \pm 1 $ & $  1 \pm 1 $ \\
		\end{tabular}
		\caption{Precision and recall for endpoints difference between true 
			\gls{tmh} and predicted \gls{tmh} below or equal to 10, 5, 2 and 0.}
		\label{tab:endpoint}
	\end{subtable}
	\caption{Results of different overlap ratios and endpoints differences comparing the different layers of the model.
	Error rates is specified as the standard deviation of the multiple runs of the experiments.}
	\label{tab:step_compare}
\end{table}

Table \ref{tab:step_compare} contains the result for different measurements 
comparing the steps in the model. Unsurprising goes both the precision and the 
recall down when the requirement for the predictions gets stricter. In 
Table \ref{tab:overlap} can it be seen that the difference between $10\%$ and $25\%$
overlap is very small and within one standard deviation from each other, 
this is implying to that if the model have made a prediction it has more than 
$25\%$ or less than $10\%$ overlap with a true \gls{tmh}.
Table \ref{tab:endpoint} shows that the changing the endpoint requirement a little 
gives a big difference in the result, from a precision on $84 \pm 2$ for an endpoints 
difference up to $10$ to a precision on $2 \pm 1$ for no endpoints difference.
Both precision and recall is very low if the requirement for a predicted \gls{tmh}
to be correct is that there is no difference in the endpoints, ie. the prediction 
is $100\%$ correct, which means there are almost no $100\%$ correct predictions.
From both measurement types can it be seen that step 2 helps a lot on especial 
the precision. This is because it removes a lot of small \glspl{tmh} from the 
prediction that does not correspond to real \glspl{tmh}, these small predicted 
\glspl{tmh} was lowering the precision but don't have much impact on the recall.
The results from step 3 is not very good, it doesn't change the result for 
$10\%$ and $25\%$ overlap, but it is still slightly worse than step 2. 
For $50\%$ overlap and endpoints difference below 5 and 10, it considerable lowers
both precision and recall and for $75\%$ overlap and endpoints difference below 
2, step 3 almost halve the results. The increasing impact step 3 has on the 
result as the requirement for the predictions gets stricter, is because step 3 
can only move the endpoints 6 positions and 6 positions don't have much impact 
if a prediction only has to have $10\%$ overlap but have a very large impact 
if the endpoint difference has to be below 2. 
Looking closer at the prediction from step 3 used to adjust the endpoints, it 
seems to give the same probability for every segment. The probabilities only
differs in the fifth decimal point and looks more like rounding inaccuracy
than predictions. The model does not seem to be able to learn any meaningful
thing from the input and have minimized the loss by always predicting the 
distribution of the labels in the training data. 
This failure to learn the patterns in the input data to accurately predict
the start end end of \glspl{tmh} could be because patterns is too complex or 
indistinct to learn with the size of the dataset or with the layout of the 
model.

\begin{table}
	\centering 
	\begin{tabular}{l|c|c} 
		Model & Precision & Recall \\ \hline
		Step 1 & $64 \pm 3$ & $66 \pm 4$ \\ 
		w PSSMs& $64 \pm 3$ & $66 \pm 4$ \\
		Step 2 & $67 \pm 3$ & $69 \pm 3$ \\ 
		w PSSMs& $67 \pm 3$ & $69 \pm 3$ \\
		Step 3 & $49 \pm 3$ & $49 \pm 4$ \\
		w PSSMs& $49 \pm 3$ & $49 \pm 4$ \\
		HMM   & $34 \pm 0$ & $13 \pm 0$ \\ 
		TMSEG\cite{tmseg} & $87 \pm 3$ & $84 \pm 3$
	\end{tabular}
    \caption{Precision and recall for 50\% overlap and endpoints difference below 5.}
	\label{tab:pr50}
\end{table}

In Table \ref{tab:pr50} is the precision and recall from using the same 
measurement as used by TMSEG. It also shows how the model compares with 
other models. TMSEG is a lot better at both precision and recall, 
but the model is a big improvement to the simple \gls{hmm}
based model. 

\subsection{Running time}
\label{sec:time}

%Inference time for step 1: 1.6142690181732178
%Inference time for step 2: 0.0045070648193359375
%Inference time for step 3: 2.752535581588745
%Training time for step 1: 800.2764098644257
%Training time for step 3: 1267.4366292953491

%Training time: 0.049510955810546875
%Decoding time: 0.8011248111724854

\begin{table}
	\centering 
	\begin{tabular}{l|c|c} 
		Model & Training & Inference \\ \hline
		Step 1 & 800 & 1.61 \\ 
		Step 2 & - & 0.005 \\ 
		Step 3 & 1267 & 2.75 \\
		HMM   & 0.0495 & 0.80
	\end{tabular}
	\caption{Training and inference timings for the different steps 
		and for a \gls{hmm}. All numbers in seconds. Training was done
		on the three first subset and inference was done on the last.}
	\label{tab:time}
\end{table}

Table \ref{tab:time} contains timings of the training and inference of each 
step of the model and of the simple \gls{hmm}. It takes a lot longer to train 
step 1 and step 3 than it does to use them to inference. 
It takes a lot longer to use the \gls{hmm} for inference than it does to train
it but it is still faster than the \glspl{lstm}. Since training only have to be
done once and the trained model can be saved and used later, the slow training 
time is not a huge disadvantages in comparison with other models. The inference
time is more important, because that is the time that is relevant if the model 
was to be used, and while it is also slower than the \gls{hmm} for inference,
it is not much slower.

\subsection{Discussion}
% What have we achieved? 
% Further work for it to be even better. 

%\subsection{Learning what the Model has Learned}
% Can we learn something from what the model has learn. By somehow visualising the weight matrices we can
% se what the model has found to be importen and sometimes something interresting aboubt the problem can be learned

\subsection{Further Work}
The model does not predict the topology of the structure, i.e. which 
non-\gls{tmh} parts are inside the membrane and which parts are outside.
Step 4 of TMSEG is the part of their model that does this and if I had more
time was this the first improvement I would add to this model.
Other interesting things to take a look at is other classes in the 
structure like signal peptides and reentrant loops. 
The model was also only trained to look at \glspl{tmp} and not 
soluble proteins and can therefore not be used to discriminate 
between those. This is something TMSEG does really well and 
something I would have looked at if I had more time.
For a more distant goal it could be interesting to examine other \gls{rnn}
architectures to see if others was more suitable to the problem and
if it was possible to improve the result by also using feature extraction.



