\section{Model}
% Hvordan har vi valgt at prøve at løse problemet.
% Vi har delt problemet op i subproblemer ligesom tmseg
% Vi har prøvet at bruge andre machine learning metoder
% Største forskel er at vi har givet model "rå" data i stedet for feature extraction
% Fordelen er at man ikke behøver at på forhånd finde ud af hvilke features er vigtig

% Hvilke muligheder er der for at implementere modelen. forskellige ml libraries
% Vi har valgt tensorflow

% Noget om hvorfor lstm
	% Der er flere beregninger når man bruger lstms (con)
	% Long short-term memory: Hvorfor har vi brug for LONG memory?
% Hvad gør vores model forskelligt fra tmseg

% Hvad forventer vi vores model kan gøre bedre end de modeller der allerede eksisterer?

% tegning af model!!

%  Problem delt op i 4 steps som i tmseg
% Kun 3 af stepsne er machine learning
	% Første step assigner en sandsynlighed for hver klasse til hver position i sekvens
		% Vi har gjort det med en lstm, fodret med den rå sekvens
		% Første lag embeder sekvens data i et vector rum
		% Andet lag er en bidirectionel lstm 
		% Trejde og sidste lag er et softmax layer som giver en sansynligheds fordeling af de forskellige klasser
		
	% Andet step er at give hver position en klasse ud fra dens sandsynlighed 
		% med forbehold for at helixer ikke må være for korte
		% Måske tilføje et bias til nogle af klasserne
		% Ikke machine learning men mere post processing af af første step
		
	% Trejde step er justering af enderne af helixerne
	
	% Fjerne step er finde en topologi af sekvens, 
		% ie hvilken vej vender helixerne og hvilke dele er inden for membranen og hvilke er uden for

In many \gls{ann} based models it is very hard to ensure some syntax rules for the output. If the output 
classes was \emph{\gls{tmh}}, \emph{inside} and \emph{outside}, then it should not be possible for a output sequence to contain 
adjacent positions with one being inside and the other being outside or both ends of a \gls{tmh} to be on the 
same side, but these rules cannot be ensured. If the problem is divided in sub problems in such a way that 
the rules is in the way the problem is divided. This is what was done in TMSEG\cite{tmseg}, they divided 
the problem such that each sub-problem is more focused on one thing and most of the desired structure is 
enforced in the design of the problem. They still do some post processing to constrain the output in certain ways, 
signal peptides is only in the beginning and \glspl{tmh} is not too short, but this is much simpler than 
somehow constraining the order the classes is allowed to appear in. The other advantages of the sub-division 
is the ability of each sub-problem to focus on one thing. In TMSEG the problem was divided into four steps
where the focus of the first step was to identify regions of the protein with \glspl{tmh} or signal peptides.
The second step's focus was post processing of the first step, to remove noise and constrain the output.
The focus of the third step was to adjust the endpoints of the \glspl{tmh} to give a more precise location.
The fourth and final step's focus was to assign a topology to the protein. 

To make this model I have chosen to use the same division of the problem as in TMSEG because of the 
advantages listed above. I chosen to use different machine learning methods to examine the feasibility 
of giving the model the raw data and letting it learn what is important and to look into which trade-offs
there is to doing it this way instead of using expert knowledge about the problem to choosing and extracting 
the important features. \glspl{lstm} is chosen as the main machine learning method because it is very suitable
to use on raw sequence data and because of good results in a lot of different problems. 

Ideally I would have looked at all steps and compared them individually with the corresponding step in TMSEG,
but due to time constraint I have concentrated on the first three steps and mainly compared them together.

\begin{figure}
	\centering
%	\includegraphics[width=\textwidth]{}
	\caption{The layers for the model of step 1}
	\label{fig:step1}
\end{figure}

The model for step 1 consists of an embedding layer with a bidirectional \gls{lstm} on top followed by a
forward \gls{lstm} and a softmax layer on top to give a probability distribution of the output classes.
An illustration of the model for step 1 can be seen in Figure \ref{fig:step1}.

The embedding layer is a matrix that transform the input from discrete integers to dense vectors, 
this serves two purposes. The first and most important purpose is to embed the discrete input values 
into a continues vector space. The input is integers from 0 to 19 corresponding to the amino acids, but 
these are discrete and have no concept of distance. By embedding them into a continues vector space 
a meaning is assigned to distance and the matrix can be altered by the training of the network, 
the idea is the network learns to embed similar amino acids to similar regions in the vector space
and with multiple dimension, different dimension could represent different measures for similarity. 
The measures for similarity is not necessarily something that makes sense to a human but just whatever 
gives the best results. The second purpose is to project the dimensionality of the input to something else 
to be able to chose the size of the next layer. This could be done by a projection layer but with an embedding 
layer this is unnecessary. 

After the embedding layer comes a bidirectional \gls{lstm}. This consists of two parallel \glspl{lstm} where 
one is an ordinary forward \gls{lstm} and the other is a backward \gls{lstm} that starts at the end of the 
input sequence and goes though it to the start. The output from the two \glspl{lstm} is then concatenated
such that the total output is a sequence of element wise concatenated output corresponding to the same input.
The reason for this layer to be bidirectional is that each element in the output now consists of information
gathered from both before and after the element's position. 

On top of the bidirectional \gls{lstm} is an ordinary forward \gls{lstm} to do some computation on the
gathered information. This layer does not benefit from being bidirectional because it already have 
information from both directions. It does however benefit from being a recurrent layer, not 
because it can remember earlier input, but because it can remember earlier output and therefore have
information about the number of consecutive output of the same class. This layer does not actually 
output the classes, but a vector used to compute the probability distribution.

The last layer is softmax layer. This is an ordinary non-recurrent fully-connected layer, same as in 
a \gls{mlp}. This layer uses the softmax function as activation function to be able to output 
a probability distribution of the output classes.
 
\todo{Beskriv step 2}

\todo{beskriv step 3}

\subsection{Machine Learning Libraries}
A lot of different machine learning libraries exists, like Tensorflow\cite{tensorflow}, Caffe\cite{caffe}, 
Theano\cite{theano} and more. These can make the implementations of a machine learning model a lot easier 
and faster. I have chosen to use Tensorflow mostly because I had prior knowledge and experience with it.

With tensorflow you first define a computation graph for the model and then you run the graph.
This has the advantages that the graph is just a data structure and therefore can be language and platform 
independent, and the defining of the computation graph and the running of it can be separated to different 
processes. The running can therefore be handled by a highly optimized backend and the defining can be done 
by using an API in a different language.
The library also adds a lot of modules that can be used together as blocks to form large parts of the 
computation graph. Most standard components of machine learning models, like different layer types, 
loss functions, training algorithms and even multiple different implementations of \gls{lstm} cells.
This gives the ability to concentrate on non standard parts of the model that is associated to the 
problem instance and not the type of machine learning used.

\subsection{Implementation}
% Hvorfor tensorflow, python
% Hvad får vi fra tensorflow, hvilket arbejde har vi selv gjort
% Hvor meget arbejde ligger der i de forskellige dele af model

% Tensorflow giver selve machine learning bygge klodserne 
% men vi har selv sat dem sammen til en model 
% For og efter behandling af dataen har der også lagt meget arbejde i